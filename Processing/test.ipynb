{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 175954 entries, 0 to 4859\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   titre    175954 non-null  object\n",
      " 1   date     175954 non-null  object\n",
      " 2   extrait  175723 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "path = \"../Data collection/Historic data/raw data/\"\n",
    "files = glob.glob(path + \"/*.csv\") \n",
    "Datasets = pd.DataFrame()\n",
    "content = [pd.read_csv(filename, index_col=None) for filename in files]\n",
    "Datasets = pd.concat(content)\n",
    "\n",
    "Datasets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\dateparser\\date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    }
   ],
   "source": [
    "def format_date(date):\n",
    "    return  dateparser.parse(str(date)).date()\n",
    "Datasets[\"date\"] = Datasets[\"date\"].apply(format_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets.to_csv(r\"C:\\Users\\hp\\Desktop\\PE\\Processing\\Outpout\\Datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>extrait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129915</th>\n",
       "      <td>Vidéo. Université d’été CGEM. Jean-Louis Borlo...</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Dans une déclaration devant Le360, en marge de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60829</th>\n",
       "      <td>Air France relance ses vols vers trois villes...</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>Air France relance ses vols vers trois villes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160768</th>\n",
       "      <td>Première rencontre entre les opérateurs du tou...</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>\\nLa ministre du Tourisme a reçu, ce mardi 2 n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170385</th>\n",
       "      <td>Maroc-Sénégal : Signature de sept accords et c...</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>\\nLe Roi Mohammed VI, accompagné du Prince Mou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107513</th>\n",
       "      <td>\"Il n’y a pas encore de découvertes de pétrole...</td>\n",
       "      <td>2014-03-14</td>\n",
       "      <td>\"Il n’y a pas encore de découvertes de pétrole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    titre        date  \\\n",
       "129915  Vidéo. Université d’été CGEM. Jean-Louis Borlo...  2019-09-13   \n",
       "60829    Air France relance ses vols vers trois villes...  2020-08-18   \n",
       "160768  Première rencontre entre les opérateurs du tou...  2021-11-02   \n",
       "170385  Maroc-Sénégal : Signature de sept accords et c...  2013-07-26   \n",
       "107513  \"Il n’y a pas encore de découvertes de pétrole...  2014-03-14   \n",
       "\n",
       "                                                  extrait  \n",
       "129915  Dans une déclaration devant Le360, en marge de...  \n",
       "60829   Air France relance ses vols vers trois villes ...  \n",
       "160768  \\nLa ministre du Tourisme a reçu, ce mardi 2 n...  \n",
       "170385  \\nLe Roi Mohammed VI, accompagné du Prince Mou...  \n",
       "107513  \"Il n’y a pas encore de découvertes de pétrole...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_data=pd.read_csv(r\"C:\\Users\\hp\\Desktop\\PE\\Processing\\Outpout\\Datasets.csv\")\n",
    "formated_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titre      0\n",
       "date       0\n",
       "extrait    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert date column into datetima\n",
    "formated_data['date'] = pd.to_datetime(formated_data['date'], errors = 'coerce')\n",
    "\n",
    "# delete rows with null values\n",
    "formated_data.dropna(inplace=True)\n",
    "formated_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_data.to_csv(r\"Outpout\\Datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "# from nltk import FreqDist\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "spacy.load('fr_core_news_md')\n",
    "import fr_core_news_md #import spacy french stemmer\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation\n",
    "\n",
    "import pyLDAvis #Nous utilisons pyLDAvis pour créer des visualisations interactives de modèles de sujet.\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            PATTERN = r'[?|$|&|*|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "\n",
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus,mostCommonsWord):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        others_sw=[\"maroc\",\"morocco\",\"marocain\",\"marocaine\",\"marocains\",\"marocaines\",\"maghreb\",\"météorologique\",\"journée\",\n",
    "                   \"méteo\",\"retweet\",\"newspic\",\"twitter\",\"com\",\"pic\",\"newspic\",\"illustration\"]\n",
    "        \n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        french_sw.extend(others_sw)\n",
    "        french_sw.extend(mostCommonsWord)\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    idx_doc=[] #liste qui va stocker les indices documents qui seront dans le corpus\n",
    "    for idx,document in enumerate(corpus):\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        \n",
    "        if len(lemms)>5: #supprime les document qui ne contient pas plus de 2 mots\n",
    "            text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "            corpus_lemms.append(text)\n",
    "            idx_doc.append(idx) #ajoute l'indice du documents\n",
    "            \n",
    "    return corpus_lemms,idx_doc\n",
    "\n",
    "\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=2, threshold=10) # higher threshold fewer phrases\n",
    "    # Un moyen plus rapide d'obtenir une phrase matraquée comme un trigramme / bigramme\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    corpus_bigram=[\" \".join(bigram_mod[doc]) for doc in texts]\n",
    "    return corpus_bigram\n",
    "\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=tokenize_text(corpus)\n",
    "    #corpus=remove_mostCommonWords(corpus,max_freq=20)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus,idx_docs=lemm_tokens(corpus)\n",
    "    \n",
    "    \n",
    "    return corpus,idx_docs\n",
    "\n",
    "\n",
    "# extracts topics with their terms and weights\n",
    "# format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    \n",
    "    #trie les indices des mots de chaque topics selon la poids du mots dans le topics\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    \n",
    "    #trie les poids des mots de chaques topics,en recuperant les poids des indices deja triée\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights,sorted_indices)])\n",
    "    \n",
    "    #recupres les mots selon leurs indices deja triée\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "    \n",
    "    #concatene chaque mots et sa poids sous formes de tuple (mot,poids)\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics\n",
    "\n",
    "\n",
    "# prints components of all the topics \n",
    "# obtained from topic modeling\n",
    "def print_topics_udf(topics, total_topics=1,weight_threshold=0.0001,display_weights=False,num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt)) for term, wt in topic] #recupere les mots et les poids du topic\n",
    "        \n",
    "        #seuillage des mots selon le seuil de poids definie\n",
    "        topic = [(word, round(wt,2)) for word, wt in topic if abs(wt) >= weight_threshold]\n",
    "        \n",
    "        #affiches les \"num_terms\" de chaque topics\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms]) if num_terms else tw\n",
    "        print()\n",
    "\n",
    "\n",
    "# # Calcul de la cohérence du modèle :\n",
    "# La cohérence des sujets constituent une mesure pratique pour juger de la qualité d'un modèle de sujet,ici on utilise La Coherence UMass qu'on a implementé nous meme.\n",
    "\n",
    "\n",
    "def getTopicTerms(pos_topics):\n",
    "    \"\"\"\n",
    "    Fonction qui retourne l'ensemble des mots qui compose chaque topics\n",
    "    ----Input----\n",
    "    pos_topics: ensemble des topics qui contients les mots et leurs poids\n",
    "    ---output---\n",
    "    topic_terms : ensemble des mots des topics\n",
    "    \n",
    "    \"\"\"\n",
    "    topic_terms=[]\n",
    "    for topic in pos_topics:\n",
    "        #topic=topic[:max_term] #recupere les \"max_term\" premiere mots et leurs poids\n",
    "        terms=[]\n",
    "        for doc in topic:\n",
    "            terms.append(doc[0]) #recupere justes les mots sans les poids\n",
    "        \n",
    "        topic_terms.append(terms) #ajoute l'ensemble des mots\n",
    "    \n",
    "    return topic_terms\n",
    "\n",
    "\n",
    "def compute_coherence_values(tfidf_train_features,feature_names,corpus,data_lemmatized,id2word,max_term=20,limit=50, start=5, step=5):\n",
    "    \"\"\"\n",
    "    Calcul la coherence UMass pour different nombre de topic\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    tfidf_train_features : features tf-idf qu'on va utiliser pour entrainer chaque model\n",
    "    feature_names : ensemble des mots contenue dans la matrice tf-idf\n",
    "    corpus: corpus de base qui contients les documents sous forme de texte\n",
    "    max_term: nombre maximal de mots qu'on va prendre pour calculé la coherence de chaque topic\n",
    "    data_lemmatized: corpus sous forme de tokens\n",
    "    id2word:vocabulaire du corpus au format de gensim\n",
    "    max_term:le nombre de termes qu'on va prendre dans chaque topic pour calculer la Coherence\n",
    "    limit : Nombre maximal de topics qu'on va tester\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    best_model : le model qui contient le plus grande coherence\n",
    "    coherence_values : Valeurs des Cohérences correspondant au modèle avec le nombre respectif de sujets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model_list = [] #listes qui va contenir les modeles tester\n",
    "    coherence_values = [] #liste qui contenir les coherences de chaque models\n",
    "    # Term Document Frequency\n",
    "    \n",
    "    common_corpus = [id2word.doc2bow(text) for text in data_lemmatized] #recupere la matrice bog of word du corpus sous le format de gensim\n",
    "   \n",
    "    #print(coherence)\n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        model=NMF(n_components=num_topics,random_state=42) #model MNF\n",
    "        model.fit(tfidf_train_features)\n",
    "        weights = model.components_ #recupere les poids\n",
    "        \n",
    "        model_list.append(model) #ajoute le model la liste des models utilisé\n",
    "        \n",
    "        \n",
    "        topics=get_topics_terms_weights(weights,feature_names)\n",
    "        \n",
    "        topic_terms=getTopicTerms(topics)#recupere les mot des de chaque topics\n",
    "        \n",
    "        topic_terms=[topics[:max_term] for topics in topic_terms] #recupere les  \"max_term\" termes avec les plus grandes poids\n",
    "        \n",
    "        #calcule du Coherence UMass\n",
    "        cm = CoherenceModel(topics=topic_terms,corpus=common_corpus, dictionary=id2word, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "    \n",
    "    idx_max=np.array(coherence_values).argmax() #recupere l'indice du model qui possede le plus grands coherence\n",
    "    best_model=model_list[idx_max] #recupere le meilleur models\n",
    "    \n",
    "\n",
    "    return best_model,coherence_values\n",
    "\n",
    "\n",
    "def topic_dominant(model,tdidf_features,corpus,topics):\n",
    "    \n",
    "    #document topic distribution :la probabilité des topics pour chaque document\n",
    "    doc_topic_dist = model.transform(tdidf_features) \n",
    "    \n",
    "    topic_num=[] #liste qui contenir le numero du topic dominant dans chaque documents\n",
    "    probs_topics=[] #liste qui va contenir les probabilités du topic dominant dans chaque documents\n",
    "    topic_keywords=[] #liste qui contenir les 5 termes les plus representative du sujet\n",
    "    text_doc=[] #liste qui va contenir le texte de chaque documents\n",
    "    \n",
    "    topic_terms=getTopicTerms(topics) #recupere les mot de chaque topics\n",
    "    \n",
    "    num_doc=[]\n",
    "    \n",
    "    for i,doc in enumerate(doc_topic_dist):\n",
    "        text_doc.append(corpus[i]) #recupere le texte du documents\n",
    "        num_doc.append(i+1) #recupere le numero du documents\n",
    "        \n",
    "        idx_max=doc.argmax() #recupere l'indice du topic qui a de la probabilité maximal\n",
    "        topic_num.append(idx_max) \n",
    "        probs_topics.append(round(doc.max(),4)) #recupere la probabilité maximal arrondis\n",
    "        \n",
    "        kw=\",\".join(topic_terms[idx_max][:5]) #recupere les mots clé du topic\n",
    "        topic_keywords.append(kw)\n",
    "        \n",
    "    \n",
    "    sent_topics_df = pd.DataFrame([num_doc,topic_num,probs_topics,topic_keywords,text_doc]).T\n",
    "    sent_topics_df.columns=[\"Num Document\",\"Topic Dominant\",\"Contrib Topic\",\"Key Word\",\"Text\"]\n",
    "    \n",
    "    return sent_topics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>extrait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transport – Royal Air Maroc, première compagni...</td>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>Le transporteur national, Royal Air Maroc (RAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Innovation – Maroc: 10 lauréats désignés lors ...</td>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>Au Maroc, 10 lauréats ont été désignés lors de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classement – Tanger Med en 6è position dans la...</td>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>Le Port de Tanger Med a été classé en 6è posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistics – Après deux années d’absence, Logis...</td>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>La 9ème édition du Salon international du tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coopération – Treize MoU d’entente portant sur...</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>Les travaux du Forum « Morocco-Israel: Connect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre       date  \\\n",
       "0  Transport – Royal Air Maroc, première compagni... 2022-05-30   \n",
       "1  Innovation – Maroc: 10 lauréats désignés lors ... 2022-05-28   \n",
       "2  Classement – Tanger Med en 6è position dans la... 2022-05-28   \n",
       "3  Logistics – Après deux années d’absence, Logis... 2022-05-28   \n",
       "4  Coopération – Treize MoU d’entente portant sur... 2022-05-26   \n",
       "\n",
       "                                             extrait  \n",
       "0  Le transporteur national, Royal Air Maroc (RAM...  \n",
       "1  Au Maroc, 10 lauréats ont été désignés lors de...  \n",
       "2  Le Port de Tanger Med a été classé en 6è posit...  \n",
       "3  La 9ème édition du Salon international du tran...  \n",
       "4  Les travaux du Forum « Morocco-Israel: Connect...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\PE\\Processing\\Outpout\\Datasets.csv\", parse_dates=['date'])\n",
    "clean.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>extrait</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156585</th>\n",
       "      <td>\\nCovid-19: Une société marocaine se reconvert...</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>Casablanca -   \\r\\n     La société marocaine \"...</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151049</th>\n",
       "      <td>Médecine: le privé déclare la guerre au public</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>La guerre entre les médecins du public et du p...</td>\n",
       "      <td>2016-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7818</th>\n",
       "      <td>L’Office des changes accélère ses process digi...</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>Objectif : 95% des demandes d’autorisations vi...</td>\n",
       "      <td>2021-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81856</th>\n",
       "      <td>Maroc : forte hausse du déficit budgétaire</td>\n",
       "      <td>2019-10-15</td>\n",
       "      <td>Maroc : forte hausse du déficit budgétaire Le ...</td>\n",
       "      <td>2019-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111171</th>\n",
       "      <td>La CGEM face aux défis et attentes des entrepr...</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    titre       date  \\\n",
       "156585  \\nCovid-19: Une société marocaine se reconvert... 2020-04-21   \n",
       "151049     Médecine: le privé déclare la guerre au public 2016-06-22   \n",
       "7818    L’Office des changes accélère ses process digi... 2021-01-21   \n",
       "81856          Maroc : forte hausse du déficit budgétaire 2019-10-15   \n",
       "111171  La CGEM face aux défis et attentes des entrepr... 2019-12-28   \n",
       "\n",
       "                                                  extrait   period  \n",
       "156585  Casablanca -   \\r\\n     La société marocaine \"...  2020-04  \n",
       "151049  La guerre entre les médecins du public et du p...  2016-06  \n",
       "7818    Objectif : 95% des demandes d’autorisations vi...  2021-01  \n",
       "81856   Maroc : forte hausse du déficit budgétaire Le ...  2019-10  \n",
       "111171  La Confédération générale des entreprises du M...  2019-12  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean['period']=clean['date'].dt.to_period('M')\n",
    "clean.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_M_Y = clean.groupby('period')['extrait'].apply(list)\n",
    "periods = clean.period.unique().tolist()\n",
    "extraits_grouped = grouped_by_M_Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-11-22 00:00:00')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpuses = [ corpus=lower_text(corpus) for corpus in extraits_grouped]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some text mining functions for processing text."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19084cb2a221719542bd94e588e69917ee95725b301fa5f2df470f319dde6836"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
